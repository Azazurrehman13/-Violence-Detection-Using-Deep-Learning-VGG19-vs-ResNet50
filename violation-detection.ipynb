{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9477810,"sourceType":"datasetVersion","datasetId":5764601},{"sourceId":441326,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":358857,"modelId":380168}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Title: Violence Detection in Videos\nThe prevalence of videos containing violent or sensitive content on social media platforms can\nhave significant negative impacts on individuals&#39; mental health. As such, it is crucial to develop\nsystems that can detect and flag such content, allowing users to be warned before viewing it.\nThis project aims to create a deep learning model capable of detecting violence in videos and\nautomatically generating a trigger warning to inform users of potential violent content.","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Paste your paths\ntrain_videos = \"/kaggle/input/project-data/Complete Dataset/train/HockeyFight\"\nval_videos = \"/kaggle/input/project-data/Complete Dataset/val/HockeyFight\"\n\n# New base dir for extracted frames\nbase_out_dir = \"/kaggle/working/frames_split\"\n\n# Classes\nclasses = ['Violent', 'NonViolent']  # Your dataset must have 2 classes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:04:45.074494Z","iopub.execute_input":"2025-06-19T09:04:45.074806Z","iopub.status.idle":"2025-06-19T09:04:45.593563Z","shell.execute_reply.started":"2025-06-19T09:04:45.074784Z","shell.execute_reply":"2025-06-19T09:04:45.592668Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"The snippet above will:\n\nCollect all video file paths from your existing train/HockeyFight and val/HockeyFight directories.\n\nDefine a writable output root (/kaggle/working/frames_split) where extracted frames will be stored.\n\nSpecify the two target classes—Violent and NonViolent—so that downstream code can split and label each video appropriately.","metadata":{}},{"cell_type":"code","source":"all_videos = []\n\n# Collect from both train and val\nfor class_name in classes:\n    for folder in [train_videos, val_videos]:\n        class_folder = os.path.join(folder, class_name)\n        if os.path.exists(class_folder):\n            video_files = list(Path(class_folder).glob(\"*.avi\"))\n            all_videos.extend([(str(f), class_name) for f in video_files])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:04:50.371171Z","iopub.execute_input":"2025-06-19T09:04:50.372276Z","iopub.status.idle":"2025-06-19T09:04:50.407993Z","shell.execute_reply.started":"2025-06-19T09:04:50.372242Z","shell.execute_reply":"2025-06-19T09:04:50.407399Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"> This block initializes an empty list, all_videos, and then walks through both your training and validation directories, looking specifically for folders named Violent or NonViolent (as defined in classes). Whenever it finds one, it gathers all .avi files in that folder, and appends a tuple of (video_path, class_name) to all_videos.\n\nBy the end of this loop, all_videos contains a complete listing of every video in your dataset—each paired with its correct label—ready for you to stratify and split into train/val/test sets.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n# Dataset root\ntrain_dir = '/kaggle/input/project-data/Complete Dataset/train'\nval_dir   = '/kaggle/input/project-data/Complete Dataset/val'\n\n# Folder → Label mapping\nfolder_to_label = {\n    'HockeyFight': 'Violent',\n    'NonFight': 'NonViolent'  # Change this if it's named differently\n}\n\n# Collect videos with correct label\nall_videos = []\n\nfor root_dir in [train_dir, val_dir]:\n    for folder_name, label in folder_to_label.items():\n        folder_path = Path(root_dir) / folder_name\n        if folder_path.exists():\n            videos = list(folder_path.glob(\"*.avi\"))\n            print(f\"Found {len(videos)} videos in {folder_path} ({label})\")\n            all_videos.extend([(str(video), label) for video in videos])\n        else:\n            print(f\"⚠️ Folder not found: {folder_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:04:54.813386Z","iopub.execute_input":"2025-06-19T09:04:54.814732Z","iopub.status.idle":"2025-06-19T09:04:54.905275Z","shell.execute_reply.started":"2025-06-19T09:04:54.814705Z","shell.execute_reply":"2025-06-19T09:04:54.904589Z"}},"outputs":[{"name":"stdout","text":"Found 400 videos in /kaggle/input/project-data/Complete Dataset/train/HockeyFight (Violent)\nFound 1000 videos in /kaggle/input/project-data/Complete Dataset/train/NonFight (NonViolent)\nFound 100 videos in /kaggle/input/project-data/Complete Dataset/val/HockeyFight (Violent)\nFound 275 videos in /kaggle/input/project-data/Complete Dataset/val/NonFight (NonViolent)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"> This snippet walks through your train and val directories, looks for the two subfolders you’ve defined—HockeyFight and NonFight—and for each one:\n\n> Checks existence of the folder (so you’re alerted if it’s missing).\n\n> Globs all .avi video files inside it.\n\n> Prints how many videos it found in each (with their assigned label).\n\n> Appends each file path plus its label (\"Violent\" or \"NonViolent\") into the all_videos list.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split into 85% train+val, 15% test\ntrain_val_videos, test_videos = train_test_split(\n    all_videos,\n    test_size=0.15,\n    stratify=[label for _, label in all_videos],\n    random_state=42\n)\n\n# From train+val, take 15% as validation\ntrain_videos, val_videos = train_test_split(\n    train_val_videos,\n    test_size=0.1765,  # 0.1765 * 85% ≈ 15% of total\n    stratify=[label for _, label in train_val_videos],\n    random_state=42\n)\n\nprint(f\"Total: {len(all_videos)}\")\nprint(f\"Train: {len(train_videos)}, Val: {len(val_videos)}, Test: {len(test_videos)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:05:00.085559Z","iopub.execute_input":"2025-06-19T09:05:00.085837Z","iopub.status.idle":"2025-06-19T09:05:00.098568Z","shell.execute_reply.started":"2025-06-19T09:05:00.085815Z","shell.execute_reply":"2025-06-19T09:05:00.097969Z"}},"outputs":[{"name":"stdout","text":"Total: 1775\nTrain: 1241, Val: 267, Test: 267\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"**Combine & Shuffle\n\nall_videos is randomly shuffled and then split so that 15% of your total videos go into test_videos, and the remaining 85% go into train_val_videos.\n\nStratification (stratify=[label for _, label in all_videos]) ensures each split preserves the same ratio of “Violent” vs. “NonViolent” as the original.\n\nTrain / Validation Split\n\nFrom the 85% “train+val” pool, a second split takes 17.65% of that subset for val_videos.\n\nSince 17.65% of 85% ≈ 15% of the total, you end up with roughly:\n\n70% train\n\n15% validation\n\n15% test\n\nFinal Counts Printout\n\nTotal: shows the overall count of videos.\n\nTrain:, Val:, and Test: report how many videos ended up in each split.**","metadata":{}},{"cell_type":"code","source":"import cv2\n\ndef extract_and_save_frames(video_list, split_name, frames_per_video=10):\n    for video_path, label in video_list:\n        output_dir = os.path.join(base_out_dir, split_name, label)\n        os.makedirs(output_dir, exist_ok=True)\n\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        step = max(1, total_frames // frames_per_video)\n        \n        i = 0\n        count = 0\n        while cap.isOpened() and count < frames_per_video:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            if i % step == 0:\n                filename = f\"{Path(video_path).stem}_frame{count}.jpg\"\n                frame_path = os.path.join(output_dir, filename)\n                cv2.imwrite(frame_path, frame)\n                count += 1\n            i += 1\n        cap.release()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:05:05.752043Z","iopub.execute_input":"2025-06-19T09:05:05.752521Z","iopub.status.idle":"2025-06-19T09:05:06.034881Z","shell.execute_reply.started":"2025-06-19T09:05:05.752500Z","shell.execute_reply":"2025-06-19T09:05:06.034313Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"This extract_and_save_frames function takes a list of (video_path, label) pairs and, for each video:\n\nCreates the output directory under /kaggle/working/frames_split/{split_name}/{label}/.\n\nOpens the video with OpenCV and reads its total frame count.\n\nComputes a uniform sampling interval (step) so you grab frames_per_video frames spread evenly across the video’s length.\n\nIterates through the video frames:\n\nWhenever the frame index i matches a sampling point (i % step == 0), it saves that frame as a JPEG named {video_stem}_frame{count}.jpg.\n\nStops after saving the desired number of frames (frames_per_video).\n\nReleases the video capture when done.1. ","metadata":{}},{"cell_type":"code","source":"extract_and_save_frames(train_videos, 'train')\nextract_and_save_frames(val_videos, 'val')\nextract_and_save_frames(test_videos, 'test')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:05:10.862164Z","iopub.execute_input":"2025-06-19T09:05:10.862443Z","iopub.status.idle":"2025-06-19T09:08:50.172639Z","shell.execute_reply.started":"2025-06-19T09:05:10.862423Z","shell.execute_reply":"2025-06-19T09:08:50.171828Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"These three lines kick off the actual frame-extraction process for each of your dataset splits:\n\nextract_and_save_frames(train_videos, 'train')\n\nGoes through every (video_path, label) in your train_videos list.\n\nFor each video, it samples a fixed number of frames (e.g. 10) evenly across its duration.\n\nSaves those frames as JPEGs under","metadata":{}},{"cell_type":"markdown","source":"extract_and_save_frames(val_videos, 'val')\n\nDoes exactly the same for your validation set, writing frames into","metadata":{}},{"cell_type":"markdown","source":"extract_and_save_frames(test_videos, 'test')\n\nProcesses your test split in the same way, saving frames in","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import RandomAffine\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n        RandomAffine(degrees=0, shear=10, translate=(0.1, 0.1)),  # Add shear for motion\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:09:11.260636Z","iopub.execute_input":"2025-06-19T09:09:11.260911Z","iopub.status.idle":"2025-06-19T09:09:11.266990Z","shell.execute_reply.started":"2025-06-19T09:09:11.260892Z","shell.execute_reply":"2025-06-19T09:09:11.266331Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Shear (RandomAffine with shear=10) simulates an off-axis camera angle or slanted viewpoint.\n\nTranslation (translate=(0.1,0.1)) shifts the image up/down or left/right by up to 10%, emulating small framing changes.\n\nCombined with your existing flips, rotations, color jitter, and random crops, this should give your model robustness to motion blur, camera shake, and lighting variations commonly found in real-world video frames.\n","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets\nfrom torch.utils.data import DataLoader\n\ntrain_dataset = datasets.ImageFolder('/kaggle/working/frames_split/train', transform=data_transforms['train'])\nval_dataset   = datasets.ImageFolder('/kaggle/working/frames_split/val', transform=data_transforms['val'])\ntest_dataset  = datasets.ImageFolder('/kaggle/working/frames_split/test', transform=data_transforms['val'])  # use same as val\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:09:16.476712Z","iopub.execute_input":"2025-06-19T09:09:16.476954Z","iopub.status.idle":"2025-06-19T09:09:16.523115Z","shell.execute_reply.started":"2025-06-19T09:09:16.476937Z","shell.execute_reply":"2025-06-19T09:09:16.522607Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"In the above code, we are loading the dataset and also add the batch size as hyperprameter","metadata":{}},{"cell_type":"code","source":"import os\n\ndef count_frames(root_dir):\n    total = 0\n    print(f\"\\nCounting frames in: {root_dir}\")\n    for split in ['train', 'val', 'test']:\n        split_path = os.path.join(root_dir, split)\n        if not os.path.exists(split_path):\n            print(f\"{split} directory not found.\")\n            continue\n        \n        split_total = 0\n        print(f\"\\n📁 {split.upper()}:\")\n\n        for class_name in os.listdir(split_path):\n            class_path = os.path.join(split_path, class_name)\n            if os.path.isdir(class_path):\n                num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n                split_total += num_images\n                print(f\"  - {class_name}: {num_images} frames\")\n\n        print(f\"🔢 Total {split} frames: {split_total}\")\n        total += split_total\n\n    print(f\"\\n✅ Overall total frames: {total}\")\n\n# Run the function\ncount_frames('/kaggle/working/frames_split')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:09:21.621886Z","iopub.execute_input":"2025-06-19T09:09:21.622625Z","iopub.status.idle":"2025-06-19T09:09:21.642278Z","shell.execute_reply.started":"2025-06-19T09:09:21.622602Z","shell.execute_reply":"2025-06-19T09:09:21.641706Z"}},"outputs":[{"name":"stdout","text":"\nCounting frames in: /kaggle/working/frames_split\n\n📁 TRAIN:\n  - NonViolent: 8910 frames\n  - Violent: 3500 frames\n🔢 Total train frames: 12410\n\n📁 VAL:\n  - NonViolent: 1920 frames\n  - Violent: 750 frames\n🔢 Total val frames: 2670\n\n📁 TEST:\n  - NonViolent: 1920 frames\n  - Violent: 750 frames\n🔢 Total test frames: 2670\n\n✅ Overall total frames: 17750\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"In the above code I have counted the all frames that are being created from the datasets as from the Train, val and Test","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:09:26.236055Z","iopub.execute_input":"2025-06-19T09:09:26.236672Z","iopub.status.idle":"2025-06-19T09:09:26.240712Z","shell.execute_reply.started":"2025-06-19T09:09:26.236647Z","shell.execute_reply":"2025-06-19T09:09:26.239909Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_classes = 2  # Violent, NonViolent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:11:14.398813Z","iopub.execute_input":"2025-06-19T09:11:14.399310Z","iopub.status.idle":"2025-06-19T09:11:14.403330Z","shell.execute_reply.started":"2025-06-19T09:11:14.399286Z","shell.execute_reply":"2025-06-19T09:11:14.402503Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class_weights = torch.tensor([1.0, 2.0]).to(device)  # Higher weight for violent class (1)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:10:14.498818Z","iopub.execute_input":"2025-06-19T09:10:14.499087Z","iopub.status.idle":"2025-06-19T09:10:14.503768Z","shell.execute_reply.started":"2025-06-19T09:10:14.499067Z","shell.execute_reply":"2025-06-19T09:10:14.502975Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n# Load pretrained ResNet50\nmodel = models.resnet50(pretrained=True)\n\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final FC layer\nnum_ftrs = model.fc.in_features\n\nmodel.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes\n\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:11:19.466753Z","iopub.execute_input":"2025-06-19T09:11:19.467421Z","iopub.status.idle":"2025-06-19T09:11:20.572796Z","shell.execute_reply.started":"2025-06-19T09:11:19.467401Z","shell.execute_reply":"2025-06-19T09:11:20.571851Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 198MB/s] \n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Loading the Pretrained Model\n\n1st load the pretrained model \n2nd Freeze all layers ","metadata":{}},{"cell_type":"code","source":"\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3) \n\nfrom torch.optim.lr_scheduler import StepLR\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:11:53.844530Z","iopub.execute_input":"2025-06-19T09:11:53.844846Z","iopub.status.idle":"2025-06-19T09:11:53.849726Z","shell.execute_reply.started":"2025-06-19T09:11:53.844827Z","shell.execute_reply":"2025-06-19T09:11:53.848759Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"torch.optim.Adam: An adaptive learning rate optimization algorithm that adjusts the learning rate for each parameter. It's widely used because of its performance and efficiency.\n\nmodel.fc.parameters(): Specifies that only the parameters of the final fully connected (fc) layer of the model will be optimized. This is common in transfer learning, where pretrained layers are frozen and only the final classifier layer is trained.\n\nlr=1e-3: Sets the initial learning rate to 0.001, which controls how much the model's parameters are updated during each step.\n\nThis sets up a learning rate scheduler that adjusts the learning rate as training progresses.\n\nStepLR: A scheduler that decreases the learning rate by a factor every few epochs.\n\noptimizer: The optimizer whose learning rate will be updated.\n\nstep_size=5: Every 5 epochs, the scheduler will adjust the learning rate.\n\ngamma=0.1: The learning rate will be multiplied by 0.1 after every step. For example:\n\nEpochs 0–4: LR = 1e-3\n\nEpochs 5–9: LR = 1e-4\n\nEpochs 10–14: LR = 1e-5, and so on.","metadata":{}},{"cell_type":"code","source":"def train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5):\n    print(\"🔒 Phase 1: Training classifier head (frozen base)\")\n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n\n        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += (preds == labels).sum().item()\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects / len(train_loader.dataset)\n\n        # Validation\n        model.eval()\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_acc = val_correct / val_total\n        print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n        scheduler.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:12:00.374002Z","iopub.execute_input":"2025-06-19T09:12:00.374289Z","iopub.status.idle":"2025-06-19T09:12:00.383118Z","shell.execute_reply.started":"2025-06-19T09:12:00.374267Z","shell.execute_reply":"2025-06-19T09:12:00.382418Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze from layer4 onward\nfor param in model.layer4.parameters():\n    param.requires_grad = True\n\n# Use lower learning rate for fine-tuning\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:12:55.994612Z","iopub.execute_input":"2025-06-19T09:12:55.995282Z","iopub.status.idle":"2025-06-19T09:12:56.000775Z","shell.execute_reply.started":"2025-06-19T09:12:55.995259Z","shell.execute_reply":"2025-06-19T09:12:56.000089Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"for param in model.parameters():\n    param.requires_grad = False\nThis loop freezes all layers of the model by setting requires_grad = False.\n\nThis ensures no gradient will be computed for these layers, and their weights will not be updated during training.\n\nCommonly used in transfer learning to preserve pretrained features.\n\n# Unfreeze from layer4 onward\nfor param in model.layer4.parameters():\n    param.requires_grad = True\nThis loop unfreezes only the last block (layer4) of the model.\n\nGradients will be computed for these parameters, allowing only layer4 to be updated during training.\n\nThis technique allows you to:\n\nUse the power of pretrained features from earlier layers.\n\nAdapt deeper layers to your specific dataset (e.g., medical images, satellite data).\n\n","metadata":{}},{"cell_type":"code","source":"def fine_tune(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n    print(\"🔓 Phase 2: Fine-tuning full model\")\n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n\n        for inputs, labels in tqdm(train_loader, desc=\"Fine-Tuning\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += (preds == labels).sum().item()\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects / len(train_loader.dataset)\n\n        # Validation\n        model.eval()\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_acc = val_correct / val_total\n        print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), 'best_resnet50_finetuned.pth')\n            print(\"✅ Best fine-tuned model saved.\")\n        scheduler.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:12:59.988916Z","iopub.execute_input":"2025-06-19T09:12:59.989187Z","iopub.status.idle":"2025-06-19T09:12:59.996976Z","shell.execute_reply.started":"2025-06-19T09:12:59.989167Z","shell.execute_reply":"2025-06-19T09:12:59.996345Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:13:05.037941Z","iopub.execute_input":"2025-06-19T09:13:05.038439Z","iopub.status.idle":"2025-06-19T09:19:34.372893Z","shell.execute_reply.started":"2025-06-19T09:13:05.038419Z","shell.execute_reply":"2025-06-19T09:19:34.371897Z"}},"outputs":[{"name":"stdout","text":"🔒 Phase 1: Training classifier head (frozen base)\n\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 776/776 [01:08<00:00, 11.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1684 | Train Acc: 0.9486 | Val Acc: 0.9805\n\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 776/776 [01:06<00:00, 11.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0647 | Train Acc: 0.9808 | Val Acc: 0.9843\n\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 776/776 [01:08<00:00, 11.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0434 | Train Acc: 0.9869 | Val Acc: 0.9850\n\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 776/776 [01:07<00:00, 11.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0320 | Train Acc: 0.9909 | Val Acc: 0.9873\n\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 776/776 [01:07<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0210 | Train Acc: 0.9952 | Val Acc: 0.9865\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)\nfine_tune(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:19:44.313281Z","iopub.execute_input":"2025-06-19T09:19:44.313623Z","iopub.status.idle":"2025-06-19T09:26:14.744651Z","shell.execute_reply.started":"2025-06-19T09:19:44.313591Z","shell.execute_reply":"2025-06-19T09:26:14.743871Z"}},"outputs":[{"name":"stdout","text":"🔓 Phase 2: Fine-tuning full model\n\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Fine-Tuning: 100%|██████████| 776/776 [01:08<00:00, 11.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0403 | Train Acc: 0.9887 | Val Acc: 0.9835\n✅ Best fine-tuned model saved.\n\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Fine-Tuning: 100%|██████████| 776/776 [01:07<00:00, 11.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0203 | Train Acc: 0.9946 | Val Acc: 0.9899\n✅ Best fine-tuned model saved.\n\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Fine-Tuning: 100%|██████████| 776/776 [01:07<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0167 | Train Acc: 0.9964 | Val Acc: 0.9906\n✅ Best fine-tuned model saved.\n\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Fine-Tuning: 100%|██████████| 776/776 [01:07<00:00, 11.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0097 | Train Acc: 0.9973 | Val Acc: 0.9869\n\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Fine-Tuning: 100%|██████████| 776/776 [01:08<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0086 | Train Acc: 0.9977 | Val Acc: 0.9918\n✅ Best fine-tuned model saved.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:26:32.711357Z","iopub.execute_input":"2025-06-19T09:26:32.711699Z","iopub.status.idle":"2025-06-19T09:26:32.716216Z","shell.execute_reply.started":"2025-06-19T09:26:32.711669Z","shell.execute_reply":"2025-06-19T09:26:32.715423Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\nimport torch\n\n# Load ResNet50 again and adjust the FC layer\nmodel = models.resnet50(pretrained=False)\nmodel.fc = nn.Linear(model.fc.in_features, 2)\nmodel.load_state_dict(torch.load(\"best_resnet50_finetuned.pth\"))\nmodel = model.to(device)\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:26:50.466741Z","iopub.execute_input":"2025-06-19T09:26:50.467187Z","iopub.status.idle":"2025-06-19T09:26:51.038942Z","shell.execute_reply.started":"2025-06-19T09:26:50.467167Z","shell.execute_reply":"2025-06-19T09:26:51.038390Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport numpy as np\n\nall_labels = []\nall_preds = []\nall_probs = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n\n        probs = torch.softmax(outputs, dim=1)\n        preds = torch.argmax(probs, dim=1)\n\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n        all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of \"Violent\" class\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:26:57.716516Z","iopub.execute_input":"2025-06-19T09:26:57.717304Z","iopub.status.idle":"2025-06-19T09:27:15.086200Z","shell.execute_reply.started":"2025-06-19T09:26:57.717274Z","shell.execute_reply":"2025-06-19T09:27:15.085484Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nreport = classification_report(all_labels, all_preds, target_names=['NonViolent', 'Violent'])\nreport_path = \"/kaggle/working/classification_report.txt\"\n\nwith open(report_path, \"w\") as f:\n    f.write(\"Classification Report\\n\")\n    f.write(\"======================\\n\")\n    f.write(report)\n\nprint(f\"✅ Classification report saved at: {report_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:28:19.663914Z","iopub.execute_input":"2025-06-19T09:28:19.664200Z","iopub.status.idle":"2025-06-19T09:28:19.683319Z","shell.execute_reply.started":"2025-06-19T09:28:19.664179Z","shell.execute_reply":"2025-06-19T09:28:19.682753Z"}},"outputs":[{"name":"stdout","text":"✅ Classification report saved at: /kaggle/working/classification_report.txt\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(5, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['NonViolent', 'Violent'],\n            yticklabels=['NonViolent', 'Violent'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\ncm_path = \"/kaggle/working/confusion_matrix.png\"\nplt.savefig(cm_path)\nplt.close()\n\nprint(f\"✅ Confusion matrix saved at: {cm_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:28:22.250928Z","iopub.execute_input":"2025-06-19T09:28:22.251575Z","iopub.status.idle":"2025-06-19T09:28:23.133651Z","shell.execute_reply.started":"2025-06-19T09:28:22.251529Z","shell.execute_reply":"2025-06-19T09:28:23.132836Z"}},"outputs":[{"name":"stdout","text":"✅ Confusion matrix saved at: /kaggle/working/confusion_matrix.png\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(all_labels, all_probs)\nauc = roc_auc_score(all_labels, all_probs)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.4f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.grid()\n\nroc_path = \"/kaggle/working/roc_curve.png\"\nplt.savefig(roc_path)\nplt.close()\n\nprint(f\"✅ ROC curve saved at: {roc_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:28:26.344361Z","iopub.execute_input":"2025-06-19T09:28:26.344905Z","iopub.status.idle":"2025-06-19T09:28:26.475425Z","shell.execute_reply.started":"2025-06-19T09:28:26.344883Z","shell.execute_reply":"2025-06-19T09:28:26.474685Z"}},"outputs":[{"name":"stdout","text":"✅ ROC curve saved at: /kaggle/working/roc_curve.png\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!pip install -q gradio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:56:14.372711Z","iopub.execute_input":"2025-06-19T08:56:14.373276Z","iopub.status.idle":"2025-06-19T08:56:24.636530Z","shell.execute_reply.started":"2025-06-19T08:56:14.373252Z","shell.execute_reply":"2025-06-19T08:56:24.635862Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import gradio as gr\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet50\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport tempfile\nimport os\nimport random\nfrom pathlib import Path\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Path to your training frames\nFRAMES_DIR = \"/kaggle/working/frames_split\"\n\ndef load_sample_frames():\n    \"\"\"Load sample frames from your training data\"\"\"\n    frames_path = Path(FRAMES_DIR)\n    sample_frames = []\n    \n    # Load samples from each class\n    for class_dir in [\"HockeyFight\", \"NonFight\"]:\n        class_path = frames_path / class_dir\n        if class_path.exists():\n            frame_files = list(class_path.glob(\"*.jpg\")) + list(class_path.glob(\"*.png\"))\n            if frame_files:\n                # Get a few random samples\n                samples = random.sample(frame_files, min(3, len(frame_files)))\n                for sample in samples:\n                    sample_frames.append((str(sample), class_dir))\n    \n    return sample_frames\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the model\ndef load_model(model_path):\n    \"\"\"Load the fine-tuned ResNet50 model\"\"\"\n    model = resnet50(pretrained=False)\n    # Binary classification: HockeyFight (Violent) vs NonFight (NonViolent)\n    model.fc = nn.Linear(model.fc.in_features, 2)\n    \n    # Load the saved weights\n    try:\n        checkpoint = torch.load(model_path, map_location=device)\n        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n            model.load_state_dict(checkpoint['model_state_dict'])\n        else:\n            model.load_state_dict(checkpoint)\n        model.to(device)\n        model.eval()\n        print(f\"Model loaded successfully from {model_path}\")\n        return model\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None\n\n# Image preprocessing - Make sure this matches your training preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\n\n# Alternative preprocessing (try if above doesn't work)\ntransform_alternative = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\n\n# Load the model\nMODEL_PATH = \"/kaggle/working/best_resnet50_finetuned.pth\"\nmodel = load_model(MODEL_PATH)\n\n# Load sample frames for testing\nsample_frames = load_sample_frames()\n\ndef extract_frames(video_path, num_frames=16, max_frames=None):\n    \"\"\"Extract frames from video for classification\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if max_frames:\n        total_frames = min(total_frames, max_frames)\n    \n    # Calculate frame indices to extract evenly distributed frames\n    if total_frames > num_frames:\n        frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n    else:\n        frame_indices = list(range(total_frames))\n    \n    frame_count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n            \n        if frame_count in frame_indices:\n            # Convert BGR to RGB\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame_rgb)\n            \n        frame_count += 1\n        \n        if len(frames) >= num_frames:\n            break\n    \n    cap.release()\n    return frames\n\ndef classify_image(image, use_alternative_transform=False):\n    \"\"\"Classify a single frame/image as Hockey Fight (Violent) or Non-Fight (NonViolent)\"\"\"\n    if model is None:\n        return {\"Error\": 1.0}, \"Model not loaded properly\"\n    \n    try:\n        # Convert to PIL Image if needed\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n        elif isinstance(image, str):\n            # If it's a file path, load the image\n            image = Image.open(image)\n        \n        # Convert to RGB if needed\n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        \n        # Choose preprocessing based on parameter\n        current_transform = transform_alternative if use_alternative_transform else transform\n        \n        # Preprocess the image\n        input_tensor = current_transform(image).unsqueeze(0).to(device)\n        \n        # Make prediction\n        with torch.no_grad():\n            outputs = model(input_tensor)\n            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n            \n            # Get prediction\n            _, predicted = torch.max(outputs, 1)\n            \n            # Check both possible class orders\n            # Option 1: 0=NonFight, 1=HockeyFight\n            result_v1 = {\n                \"NonViolent (NonFight)\": float(probabilities[0].item()),\n                \"Violent (HockeyFight)\": float(probabilities[1].item())\n            }\n            \n            # Option 2: 0=HockeyFight, 1=NonFight (in case class order is different)\n            result_v2 = {\n                \"NonViolent (NonFight)\": float(probabilities[1].item()),\n                \"Violent (HockeyFight)\": float(probabilities[0].item())\n            }\n            \n            # Determine classification for both interpretations\n            pred_class_v1 = \"NonViolent (NonFight)\" if predicted.item() == 0 else \"Violent (HockeyFight)\"\n            conf_v1 = float(probabilities[predicted.item()].item())\n            \n            pred_class_v2 = \"Violent (HockeyFight)\" if predicted.item() == 0 else \"NonViolent (NonFight)\"\n            conf_v2 = float(probabilities[1-predicted.item()].item())\n            \n            # Return both interpretations for debugging\n            classification_text = f\"\"\"\n🎯 **Raw Model Output**: Class {predicted.item()} with confidence {conf_v1:.2%}\n\n**Interpretation 1** (0=NonFight, 1=HockeyFight):\n- Classification: {pred_class_v1}\n- Confidence: {conf_v1:.2%}\n\n**Interpretation 2** (0=HockeyFight, 1=NonFight):\n- Classification: {pred_class_v2}\n- Confidence: {conf_v2:.2%}\n\n**Raw Probabilities**: [{probabilities[0].item():.3f}, {probabilities[1].item():.3f}]\n**Transform Used**: {'Alternative' if use_alternative_transform else 'Standard'}\n            \"\"\"\n            \n            return result_v1, classification_text.strip()\n            \n    except Exception as e:\n        return {\"Error\": 1.0}, f\"Error during classification: {str(e)}\"\n\ndef classify_training_frame(frame_path):\n    \"\"\"Classify one of your actual training frames\"\"\"\n    if not frame_path:\n        return {\"\": 0}, \"No frame selected\"\n    \n    try:\n        # Get the true label from the path\n        true_label = \"HockeyFight\" if \"HockeyFight\" in frame_path else \"NonFight\"\n        \n        # Classify with both transforms\n        result1, details1 = classify_image(frame_path, use_alternative_transform=False)\n        result2, details2 = classify_image(frame_path, use_alternative_transform=True)\n        \n        analysis_text = f\"\"\"\n📁 **Training Frame Analysis**\n🏷️ **True Label**: {true_label}\n📍 **File**: {os.path.basename(frame_path)}\n\n**Standard Transform Results:**\n{details1}\n\n**Alternative Transform Results:**\n{details2}\n\n**Recommendation**: Compare both results with the true label to see which transform/interpretation works better.\n        \"\"\"\n        \n        return result1, analysis_text.strip()\n        \n    except Exception as e:\n        return {\"Error\": 1.0}, f\"Error: {str(e)}\"\n\ndef classify_video(video_path, classification_method=\"average\"):\n    \"\"\"Classify a video by extracting frames and analyzing each frame individually\"\"\"\n    if model is None:\n        return \"Error: Model not loaded properly\", \"Model Error\"\n    \n    if video_path is None:\n        return \"No video uploaded\", \"No Input\"\n    \n    try:\n        # Extract frames from video (similar to training data preparation)\n        frames = extract_frames(video_path, num_frames=16)\n        \n        if not frames:\n            return \"Could not extract frames from video\", \"Frame Extraction Error\"\n        \n        # Process each frame individually (like training data)\n        frame_predictions = []\n        frame_probabilities = []\n        \n        for i, frame in enumerate(frames):\n            # Convert to PIL Image\n            pil_frame = Image.fromarray(frame)\n            \n            # Classify this frame (same as training approach)\n            result, _ = classify_image(pil_frame)\n            \n            if \"Error\" not in result:\n                non_violent_prob = result[\"NonViolent (NonFight)\"]\n                violent_prob = result[\"Violent (HockeyFight)\"]\n                \n                frame_probabilities.append([non_violent_prob, violent_prob])\n                frame_predictions.append(1 if violent_prob > non_violent_prob else 0)\n        \n        if not frame_probabilities:\n            return {\"Error\": 1.0}, \"No frames could be processed\"\n        \n        # Aggregate frame predictions\n        frame_probabilities = np.array(frame_probabilities)\n        \n        if classification_method == \"average\":\n            # Average probabilities across all frames\n            avg_probabilities = np.mean(frame_probabilities, axis=0)\n            final_prediction = np.argmax(avg_probabilities)\n        elif classification_method == \"majority\":\n            # Majority vote across frames\n            final_prediction = np.bincount(frame_predictions).argmax()\n            avg_probabilities = np.mean(frame_probabilities, axis=0)\n        else:\n            # Max confidence approach\n            max_conf_idx = np.argmax(np.max(frame_probabilities, axis=1))\n            final_prediction = frame_predictions[max_conf_idx]\n            avg_probabilities = frame_probabilities[max_conf_idx]\n        \n        # Create result dictionary\n        result = {\n            \"NonViolent (NonFight)\": float(avg_probabilities[0]),\n            \"Violent (HockeyFight)\": float(avg_probabilities[1])\n        }\n        \n        # Final classification\n        class_names = [\"NonViolent (NonFight)\", \"Violent (HockeyFight)\"]\n        predicted_class = class_names[final_prediction]\n        confidence = float(avg_probabilities[final_prediction])\n        \n        # Detailed analysis\n        num_frames_processed = len(frames)\n        violent_frames = sum(1 for pred in frame_predictions if pred == 1)\n        \n        classification_text = f\"\"\"\n        🎯 **Final Classification**: {predicted_class}\n        📊 **Confidence**: {confidence:.2%}\n        🎬 **Frames Analyzed**: {num_frames_processed}\n        ⚡ **Violent Frames**: {violent_frames}/{num_frames_processed} ({violent_frames/num_frames_processed*100:.1f}%)\n        📈 **Aggregation Method**: {classification_method.title()}\n        \n        **Frame-by-Frame Analysis:**\n        Each frame was classified individually (same as training approach),\n        then results were aggregated for final video classification.\n        \"\"\"\n        \n        return result, classification_text.strip()\n        \n    except Exception as e:\n        error_msg = f\"Error during video classification: {str(e)}\"\n        return {\"Error\": 1.0}, error_msg\n\n# Create Gradio interface\ndef create_interface():\n    with gr.Blocks(title=\"Hockey Fight Detection\", theme=gr.themes.Soft()) as demo:\n        gr.Markdown(\n            \"\"\"\n            # 🏒 Hockey Fight Detection System\n            \n            Upload images/videos OR test with your actual training frames to debug classification issues.\n            This interface helps identify preprocessing or class mapping problems.\n            \n            **Supported formats**: Images (.jpg, .png) and Videos (.avi, .mp4, .mov, .mkv)\n            **Debug Mode**: Test with actual training frames to verify model behavior\n            \"\"\"\n        )\n        \n        with gr.Row():\n            with gr.Column(scale=1):\n                # Tabs for different input types\n                with gr.Tabs():\n                    with gr.TabItem(\"🖼️ Upload Image/Frame\"):\n                        image_input = gr.Image(\n                            label=\"Upload Hockey Frame/Image\",\n                            type=\"pil\",\n                            height=300\n                        )\n                        use_alt_transform = gr.Checkbox(\n                            label=\"Use Alternative Transform (Resize→CenterCrop)\",\n                            value=False\n                        )\n                        image_submit_btn = gr.Button(\"🔍 Classify Frame\", variant=\"primary\")\n                    \n                    with gr.TabItem(\"🎯 Test Training Frames\"):\n                        if sample_frames:\n                            training_frame_dropdown = gr.Dropdown(\n                                choices=[f\"{os.path.basename(path)} ({label})\" for path, label in sample_frames],\n                                label=\"Select a Training Frame\",\n                                value=f\"{os.path.basename(sample_frames[0][0])} ({sample_frames[0][1]})\" if sample_frames else None\n                            )\n                            training_frame_btn = gr.Button(\"🔍 Test Training Frame\", variant=\"secondary\")\n                        else:\n                            gr.Markdown(\"⚠️ No training frames found in `/kaggle/working/frames_split`\")\n                    \n                    with gr.TabItem(\"🎬 Full Video\"):\n                        video_input = gr.Video(\n                            label=\"Upload Hockey Video\",\n                            height=300\n                        )\n                        method_selector = gr.Radio(\n                            choices=[\"average\", \"majority\", \"max_confidence\"],\n                            value=\"average\",\n                            label=\"Frame Aggregation Method\",\n                            info=\"How to combine individual frame predictions\"\n                        )\n                        video_submit_btn = gr.Button(\"🔍 Analyze Video\", variant=\"primary\")\n                \n            with gr.Column(scale=1):\n                # Probability output\n                prob_output = gr.Label(\n                    label=\"Classification Probabilities\",\n                    num_top_classes=2\n                )\n                \n                # Text output for detailed results\n                text_output = gr.Textbox(\n                    label=\"Detailed Analysis & Debug Info\",\n                    lines=12,\n                    max_lines=20\n                )\n        \n        # Information sections\n        with gr.Row():\n            with gr.Column():\n                with gr.Accordion(\"🔧 Debugging Tips\", open=True):\n                    gr.Markdown(\n                        \"\"\"\n                        **Common Issues & Solutions:**\n                        1. **Wrong Class Order**: Model might have 0=HockeyFight, 1=NonFight (check both interpretations)\n                        2. **Preprocessing Mismatch**: Try both Standard and Alternative transforms\n                        3. **Class Mapping**: Verify which class index corresponds to which label\n                        \n                        **Test Strategy:**\n                        - Use \"Test Training Frames\" tab to verify model works on known data\n                        - Compare results with true labels to identify the issue\n                        - Check raw probabilities and model outputs\n                        \"\"\"\n                    )\n                    \n            with gr.Column():\n                with gr.Accordion(\"ℹ️ Model Information\", open=False):\n                    gr.Markdown(\n                        \"\"\"\n                        **Training Approach**: \n                        - Model trained on individual frames extracted from hockey videos\n                        - Each frame labeled as HockeyFight or NonFight\n                        - ResNet50 architecture fine-tuned for frame-level classification\n                        \n                        **Classes**: \n                        - NonViolent (NonFight): Regular hockey gameplay frames\n                        - Violent (HockeyFight): Fighting/aggressive behavior frames\n                        \n                        **Input**: 224x224 RGB images (extracted video frames)\n                        **Video Processing**: Extracts frames → Classifies each → Aggregates results\n                        \"\"\"\n                    )\n        \n        with gr.Accordion(\"⚠️ Usage Guidelines\", open=False):\n            gr.Markdown(\n                \"\"\"\n                **Best Results:**\n                - Clear hockey game footage\n                - Good lighting and resolution\n                - Videos under 2 minutes for faster processing\n                \n                **Limitations:**\n                - Designed specifically for hockey content\n                - May not work well on other sports or contexts\n                - Results should be interpreted by domain experts\n                \n                **Disclaimer**: This model is for research and educational purposes only.\n                \"\"\"\n            )\n        \n        # Event handlers\n        image_submit_btn.click(\n            fn=lambda img, alt_transform: classify_image(img, alt_transform),\n            inputs=[image_input, use_alt_transform],\n            outputs=[prob_output, text_output]\n        )\n        \n        if sample_frames:\n            training_frame_btn.click(\n                fn=lambda selected: classify_training_frame(\n                    next((path for path, label in sample_frames \n                         if f\"{os.path.basename(path)} ({label})\" == selected), None)\n                ),\n                inputs=training_frame_dropdown,\n                outputs=[prob_output, text_output]\n            )\n        \n        video_submit_btn.click(\n            fn=lambda video, method: classify_video(video, method),\n            inputs=[video_input, method_selector],\n            outputs=[prob_output, text_output]\n        )\n        \n        # Auto-classify on upload\n        image_input.change(\n            fn=lambda img, alt_transform: classify_image(img, alt_transform) if img else ({}, \"\"),\n            inputs=[image_input, use_alt_transform],\n            outputs=[prob_output, text_output]\n        )\n    \n    return demo\n\n# Launch the interface\nif __name__ == \"__main__\":\n    demo = create_interface()\n    # Kaggle-friendly launch configuration\n    demo.launch(\n        share=True,  # Create a public link for Kaggle\n        debug=False,  # Disable debug in Kaggle\n        quiet=True,   # Reduce console output\n        height=700,   # Set height for Kaggle display\n        show_error=True\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:30:45.568762Z","iopub.execute_input":"2025-06-19T09:30:45.569593Z","iopub.status.idle":"2025-06-19T09:30:47.276088Z","shell.execute_reply.started":"2025-06-19T09:30:45.569559Z","shell.execute_reply":"2025-06-19T09:30:47.275200Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully from /kaggle/working/best_resnet50_finetuned.pth\n* Running on public URL: https://70738e541d03d1a354.gradio.live\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://70738e541d03d1a354.gradio.live\" width=\"100%\" height=\"700\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":32}]}